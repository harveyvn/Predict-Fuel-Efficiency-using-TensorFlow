{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-10-17T17:48:44.767852Z","iopub.execute_input":"2021-10-17T17:48:44.768202Z","iopub.status.idle":"2021-10-17T17:48:44.775144Z","shell.execute_reply.started":"2021-10-17T17:48:44.768158Z","shell.execute_reply":"2021-10-17T17:48:44.773816Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make NumPy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n# Turn off the warning altogether\npd.set_option('mode.chained_assignment',None)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:47.380627Z","iopub.execute_input":"2021-10-17T17:48:47.380920Z","iopub.status.idle":"2021-10-17T17:48:47.387057Z","shell.execute_reply.started":"2021-10-17T17:48:47.380886Z","shell.execute_reply":"2021-10-17T17:48:47.385903Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:47.794858Z","iopub.execute_input":"2021-10-17T17:48:47.795721Z","iopub.status.idle":"2021-10-17T17:48:47.800899Z","shell.execute_reply.started":"2021-10-17T17:48:47.795665Z","shell.execute_reply":"2021-10-17T17:48:47.799980Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Visualize the model's training progress","metadata":{}},{"cell_type":"code","source":"def plot_loss(history):\n  plt.plot(history.history['loss'], label='loss')\n  plt.plot(history.history['val_loss'], label='val_loss')\n  plt.ylim([0, 10])\n  plt.xlabel('Epoch')\n  plt.ylabel('Error [MPG]')\n  plt.legend()\n  plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:49.028632Z","iopub.execute_input":"2021-10-17T17:48:49.029356Z","iopub.status.idle":"2021-10-17T17:48:49.035344Z","shell.execute_reply.started":"2021-10-17T17:48:49.029308Z","shell.execute_reply":"2021-10-17T17:48:49.034377Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# The Auto MPG dataset\nThe dataset is available from the UCI [Machine Learning Repository](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data).\n\n## Get the data\nFirst download and import the dataset using pandas:","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nurl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\nnames = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\"acceleration\", \"model year\", \"origin\", \"car name\"]\nwidths = [7, 4, 10, 10, 11, 7, 4, 4, 30]\n\n# Get the data\nX_full = pd.read_fwf(url, names=names, widths=widths, na_values=['?'])\nX = X_full.copy()\nX.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:50.085700Z","iopub.execute_input":"2021-10-17T17:48:50.086663Z","iopub.status.idle":"2021-10-17T17:48:50.313182Z","shell.execute_reply.started":"2021-10-17T17:48:50.086611Z","shell.execute_reply":"2021-10-17T17:48:50.312200Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\nCreate a new feature named \"company\" basing on \"car name\"","metadata":{}},{"cell_type":"code","source":"# A dictionary of companies getting from a feature \"car name\"\nbrands_dict = {\n    \"amc\": \"AMC\",\n    \"audi\": \"Audi\",\n    \"bmw\": \"Bmw\",\n    \"buick\": \"Buick\",\n    \"cadillac\": \"Cadillac\",\n    \"capri\": \"Capri\",\n    \"chevroelt\": \"Chevrolet\",\n    \"chevrolet\": \"Chevrolet\",\n    \"chevy\": \"Chevrolet\",\n    \"chrysler\": \"Chrysler\",\n    \"datsun\": \"Datsun\",\n    \"dodge\": \"Dodge\",\n    \"fiat\": \"Fiat\",\n    \"ford\": \"Ford\",\n    \"hi\": \"IH\",\n    \"honda\": \"Honda\",\n    \"maxda\": \"Mazda\",\n    \"mazda\": \"Mazda\",\n    \"mercedes\": \"Mercedes-Benz\",\n    \"mercedes-benz\": \"Mercedes-Benz\",\n    \"mercury\": \"Mercury\",\n    \"nissan\": \"Nissan\",\n    \"oldsmobile\": \"Oldsmobile\",\n    \"opel\": \"Opel\",\n    \"peugeot\": \"Peugeot\",\n    \"plymouth\": \"Plymouth\",\n    \"pontiac\": \"Pontiac\",\n    \"renault\": \"Renault\",\n    \"saab\": \"Saab\",\n    \"subaru\": \"Subaru\",\n    \"toyota\": \"Toyota\",\n    \"toyouta\": \"Toyota\",\n    \"triumph\": \"Triumph\",\n    \"vokswagen\": \"Volkswagen\",\n    \"volkswagen\": \"Volkswagen\",\n    \"volvo\": \"Volvo\",\n    \"vw\": \"Volkswagen\"\n}\n\n\n# Create a new feature named Company\nX[\"company\"] = [brands_dict[X[\"car name\"][i].replace('\"', '').split()[0]] for i in range(len(X[\"car name\"]))]\nX.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:52.000758Z","iopub.execute_input":"2021-10-17T17:48:52.001647Z","iopub.status.idle":"2021-10-17T17:48:52.028300Z","shell.execute_reply.started":"2021-10-17T17:48:52.001606Z","shell.execute_reply":"2021-10-17T17:48:52.027676Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Data Correlation\n\nCorrelation map to see how features are correlated with each other and with \"mpg\".","metadata":{}},{"cell_type":"code","source":"corr_matrix = X_full.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corr_matrix, vmax=0.9, square=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:54.233506Z","iopub.execute_input":"2021-10-17T17:48:54.234218Z","iopub.status.idle":"2021-10-17T17:48:54.593435Z","shell.execute_reply.started":"2021-10-17T17:48:54.234168Z","shell.execute_reply":"2021-10-17T17:48:54.592519Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Look at some correlation values in a list format.","metadata":{}},{"cell_type":"code","source":"print(corr_matrix[\"mpg\"].sort_values(ascending=False)[:5], '\\n')\nprint(corr_matrix[\"mpg\"].sort_values(ascending=False)[-5:])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:56.802432Z","iopub.execute_input":"2021-10-17T17:48:56.803166Z","iopub.status.idle":"2021-10-17T17:48:56.811975Z","shell.execute_reply.started":"2021-10-17T17:48:56.803123Z","shell.execute_reply":"2021-10-17T17:48:56.811250Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the columns \"displacement\" and \"weight\" are strongly negatively correlated.\n\nWe can guess if we predict the \"mpg\" value from given columns, the \"displacement\" and \"weight\" will predict better than others.","metadata":{}},{"cell_type":"markdown","source":"# Split features from labels\n\nSeparate the target value—the \"label\"—from the features. This label is the value that you will train the model to predict.","metadata":{}},{"cell_type":"code","source":"# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=[\"mpg\"], inplace=True)\n\ny = X[\"mpg\"]\nX.drop([\"mpg\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:48:59.300771Z","iopub.execute_input":"2021-10-17T17:48:59.301629Z","iopub.status.idle":"2021-10-17T17:48:59.310909Z","shell.execute_reply.started":"2021-10-17T17:48:59.301577Z","shell.execute_reply":"2021-10-17T17:48:59.309947Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Split the data into training and test sets\n\nNow, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:01.502420Z","iopub.execute_input":"2021-10-17T17:49:01.502717Z","iopub.status.idle":"2021-10-17T17:49:01.510556Z","shell.execute_reply.started":"2021-10-17T17:49:01.502681Z","shell.execute_reply":"2021-10-17T17:49:01.509620Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Clean tha data\n\nIdentify columns with missing values","metadata":{}},{"cell_type":"code","source":"print(X_train.isna().sum())\nprint(X_valid.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:04.376375Z","iopub.execute_input":"2021-10-17T17:49:04.377134Z","iopub.status.idle":"2021-10-17T17:49:04.387858Z","shell.execute_reply.started":"2021-10-17T17:49:04.377082Z","shell.execute_reply":"2021-10-17T17:49:04.386716Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"The column \"horsepower\" contains missing value.\n\nUse SimpleImputer to replace missing values with the mean value and create a new column name \"hp\".","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation\nhp_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\nimputed_hp_train = hp_imputer.fit(X_train[[\"horsepower\"]])\nimputed_hp_valid = hp_imputer.fit(X_valid[[\"horsepower\"]])\n\n# Put them back to X_train and X_valid dataframe\nX_train[\"hp\"] = imputed_hp_train.transform(X_train[[\"horsepower\"]]).ravel()\nX_valid[\"hp\"] = imputed_hp_valid.transform(X_valid[[\"horsepower\"]]).ravel()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:07.314281Z","iopub.execute_input":"2021-10-17T17:49:07.314594Z","iopub.status.idle":"2021-10-17T17:49:07.332394Z","shell.execute_reply.started":"2021-10-17T17:49:07.314559Z","shell.execute_reply":"2021-10-17T17:49:07.331436Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"print(X_train.isna().sum())\nprint(X_valid.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:11.747120Z","iopub.execute_input":"2021-10-17T17:49:11.747454Z","iopub.status.idle":"2021-10-17T17:49:11.759286Z","shell.execute_reply.started":"2021-10-17T17:49:11.747418Z","shell.execute_reply":"2021-10-17T17:49:11.758305Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"The \"origin\" column is categorical, not numeric. \n\nSo the next step is to one-hot encode the values in the column with OneHotEncoder class from scikit-learn.","metadata":{}},{"cell_type":"code","source":"X_train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:14.779566Z","iopub.execute_input":"2021-10-17T17:49:14.779865Z","iopub.status.idle":"2021-10-17T17:49:14.799650Z","shell.execute_reply.started":"2021-10-17T17:49:14.779827Z","shell.execute_reply":"2021-10-17T17:49:14.798765Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[[\"origin\"]]))\nOH_cols_valid = pd.DataFrame(OH_encoder.fit_transform(X_valid[[\"origin\"]]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Add one-hot encoded columns to numerical features\nX_train = pd.concat([X_train, OH_cols_train], axis=1)\nX_valid = pd.concat([X_valid, OH_cols_valid], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:18.166627Z","iopub.execute_input":"2021-10-17T17:49:18.166923Z","iopub.status.idle":"2021-10-17T17:49:18.181016Z","shell.execute_reply.started":"2021-10-17T17:49:18.166892Z","shell.execute_reply":"2021-10-17T17:49:18.180231Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"X_train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:21.547766Z","iopub.execute_input":"2021-10-17T17:49:21.548135Z","iopub.status.idle":"2021-10-17T17:49:21.571866Z","shell.execute_reply.started":"2021-10-17T17:49:21.548091Z","shell.execute_reply":"2021-10-17T17:49:21.571064Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"X_valid.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:49:23.297599Z","iopub.execute_input":"2021-10-17T17:49:23.297911Z","iopub.status.idle":"2021-10-17T17:49:23.320127Z","shell.execute_reply.started":"2021-10-17T17:49:23.297877Z","shell.execute_reply":"2021-10-17T17:49:23.319015Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"The \"company\" is a categorical variable with high cardinality.\n\nIt is not wise to use one-hot encoding since it can create very high dimensionality, instead we will use Target-based Encoding.","metadata":{}},{"cell_type":"code","source":"from category_encoders import TargetEncoder\n\nencoder = TargetEncoder()\nencoded_comp_train = encoder.fit(X_train[[\"company\"]], y_train)\nencoded_comp_valid = encoder.fit(X_valid[[\"company\"]], y_valid)\n\n# Put them back to X_train and X_valid dataframe\nX_train[\"company_encode\"] = encoded_comp_train.transform(X_train[[\"company\"]])\nX_valid[\"company_encode\"] = encoded_comp_valid.transform(X_valid[[\"company\"]])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:51:45.036390Z","iopub.execute_input":"2021-10-17T17:51:45.036974Z","iopub.status.idle":"2021-10-17T17:51:45.081685Z","shell.execute_reply.started":"2021-10-17T17:51:45.036920Z","shell.execute_reply":"2021-10-17T17:51:45.081028Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"X_train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:51:54.070943Z","iopub.execute_input":"2021-10-17T17:51:54.071609Z","iopub.status.idle":"2021-10-17T17:51:54.094600Z","shell.execute_reply.started":"2021-10-17T17:51:54.071550Z","shell.execute_reply":"2021-10-17T17:51:54.093482Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"X_valid.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:52:04.330679Z","iopub.execute_input":"2021-10-17T17:52:04.331137Z","iopub.status.idle":"2021-10-17T17:52:04.353334Z","shell.execute_reply.started":"2021-10-17T17:52:04.331089Z","shell.execute_reply":"2021-10-17T17:52:04.352613Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Select features we will use to train the model","metadata":{}},{"cell_type":"code","source":"features = [\"cylinders\", \"displacement\", \"weight\", \"acceleration\", \"model year\", \"hp\", 0, 1, 2, \"company_encode\"]\nX_train[features].tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:52:44.119034Z","iopub.execute_input":"2021-10-17T17:52:44.119321Z","iopub.status.idle":"2021-10-17T17:52:44.139622Z","shell.execute_reply.started":"2021-10-17T17:52:44.119288Z","shell.execute_reply":"2021-10-17T17:52:44.138915Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# Normalization\n\nIt is good practice to normalize features that use different scales and ranges.\n\nThe ```tf.keras.layers.Normalization``` is a clean and simple way to add feature normalization into your model.","metadata":{}},{"cell_type":"markdown","source":"# Linear Regression\n\n## Linear regression with one variable\n\nBegin with a single-variable linear regression to predict \"mpg\" \nfrom \"cylinders\", \"displacement\", \"weight\", \"acceleration\", \"model year\", \"hp\", one by one.","metadata":{}},{"cell_type":"code","source":"%%time\ndef linear_regression_single(column):\n    # Create a numpy array made of the feature with column name \n    feature = np.array(X_train[column])\n    # Init the tf.keras.layers.Normalization\n    feature_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n    # Fit the state of the preprocessing layer to the horsepower data\n    feature_normalizer.adapt(feature)\n    \n    # Build the Keras Sequential model\n    model = tf.keras.Sequential([\n        feature_normalizer,\n        layers.Dense(units=1)\n    ])\n    \n    # Configure the training procedure using the Keras Model.compile method\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n        loss=\"mean_absolute_error\"\n    )\n\n    # Execute the training for 100 epochs\n    history = model.fit(\n        X_train[column], y_train, epochs=100, verbose=0, validation_split=0.2\n    )\n    print(\"Finish \", column)\n\n    return model, history\n\ntest_results = {}\nfor f in [\"cylinders\", \"displacement\", \"weight\", \"acceleration\", \"model year\", \"hp\", \"company_encode\"]:\n    feature_model, history = linear_regression_single(f)\n    test_results[f+\"_model\"] = feature_model.evaluate(\n        X_valid[f],\n        y_valid, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:53:28.000503Z","iopub.execute_input":"2021-10-17T17:53:28.001355Z","iopub.status.idle":"2021-10-17T17:54:01.697073Z","shell.execute_reply.started":"2021-10-17T17:53:28.001311Z","shell.execute_reply":"2021-10-17T17:54:01.696152Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Back to our first assumption, the \"displacement\" and \"weight\" features have a better score than others.","metadata":{}},{"cell_type":"code","source":"test_results","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:54:35.895095Z","iopub.execute_input":"2021-10-17T17:54:35.895885Z","iopub.status.idle":"2021-10-17T17:54:35.903783Z","shell.execute_reply.started":"2021-10-17T17:54:35.895829Z","shell.execute_reply":"2021-10-17T17:54:35.903013Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Linear regression with multiple inputs","metadata":{}},{"cell_type":"code","source":"%%time\ndef linear_regression_multi(columns):\n    # Create a numpy array made of the feature with column name \n    features = np.array(X_train[columns])\n    # Create the tf.keras.layers.Normalization\n    features_normalizer = layers.Normalization(axis=-1)\n    # Fit the state of the preprocessing layer to the horsepower data\n    features_normalizer.adapt(features)\n    \n    # Build the Keras Sequential model\n    model = tf.keras.Sequential([\n        features_normalizer,\n        layers.Dense(units=1)\n    ])\n    \n    # Configure the training procedure using the Keras Model.compile method\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n        loss=\"mean_absolute_error\"\n    )\n\n    # Execute the training for 100 epochs\n    history = model.fit(\n        X_train[columns], y_train, epochs=100, verbose=0, validation_split=0.2\n    )\n\n    return model, history\n\n\nmulti_model, history = linear_regression_multi(features)                                                 \ntest_results[\"multi_model\"] = multi_model.evaluate(\n    X_valid[features],\n    y_valid, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:54:52.380805Z","iopub.execute_input":"2021-10-17T17:54:52.381148Z","iopub.status.idle":"2021-10-17T17:54:57.287515Z","shell.execute_reply.started":"2021-10-17T17:54:52.381110Z","shell.execute_reply":"2021-10-17T17:54:57.286554Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Regression with a deep neural network (DNN)\nThese models will contain a few more layers than the linear model:\n\n- The normalization layer, as before (with horsepower_normalizer for a single-input model and normalizer for a multiple-input model).\n\n- Two hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity.\n\n- A linear Dense single-output layer.","metadata":{}},{"cell_type":"code","source":"%%time\ndef dnn_regression_multi(columns):\n    # Create a numpy array made of the feature with column name \n    features = np.array(X_train[columns])\n    # Create the tf.keras.layers.Normalization\n    features_normalizer = layers.Normalization(axis=-1)\n    # Fit the state of the preprocessing layer to the horsepower data\n    features_normalizer.adapt(features)\n    \n    # Build the Keras Sequential model\n    model = tf.keras.Sequential([\n        features_normalizer,\n        layers.Dense(64, activation='relu'),\n        layers.Dense(64, activation='relu'),        \n        layers.Dense(units=1)\n    ])\n    \n    # Configure the training procedure using the Keras Model.compile method\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n        loss=\"mean_absolute_error\"\n    )\n\n    # Execute the training for 100 epochs\n    history = model.fit(\n        X_train[columns], y_train, epochs=100, verbose=0, validation_split=0.2\n    )\n\n    return model, history\n\n\ndnn_model, history = dnn_regression_multi(features)                                                 \ntest_results[\"dnn_model\"] = dnn_model.evaluate(\n    X_valid[features],\n    y_valid, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:56:00.047099Z","iopub.execute_input":"2021-10-17T17:56:00.047443Z","iopub.status.idle":"2021-10-17T17:56:05.557051Z","shell.execute_reply.started":"2021-10-17T17:56:00.047403Z","shell.execute_reply":"2021-10-17T17:56:05.556091Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Performance\n\nSince all models have been trained, you can review their test set performance.\n\nThe \"dnn_model\" has the highest score which indicates it the best model amongs the stars.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(test_results, index=['Mean absolute error [mpg]']).T","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:56:08.209807Z","iopub.execute_input":"2021-10-17T17:56:08.210174Z","iopub.status.idle":"2021-10-17T17:56:08.224189Z","shell.execute_reply.started":"2021-10-17T17:56:08.210136Z","shell.execute_reply":"2021-10-17T17:56:08.223169Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"predictions = dnn_model.predict(X_valid[features]).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(y_valid, predictions)\nplt.xlabel('True Values [MPG]')\nplt.ylabel('Predictions [MPG]')\nlims = [0, 50]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:56:11.058655Z","iopub.execute_input":"2021-10-17T17:56:11.058946Z","iopub.status.idle":"2021-10-17T17:56:11.424117Z","shell.execute_reply.started":"2021-10-17T17:56:11.058914Z","shell.execute_reply":"2021-10-17T17:56:11.422955Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"error = predictions - y_valid\nplt.hist(error, bins=25)\nplt.xlabel('Prediction Error [MPG]')\n_ = plt.ylabel('Count')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:56:13.071643Z","iopub.execute_input":"2021-10-17T17:56:13.072377Z","iopub.status.idle":"2021-10-17T17:56:13.351650Z","shell.execute_reply.started":"2021-10-17T17:56:13.072325Z","shell.execute_reply":"2021-10-17T17:56:13.350772Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### Save it for later use","metadata":{}},{"cell_type":"code","source":"dnn_model.save('dnn_model')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:56:15.270078Z","iopub.execute_input":"2021-10-17T17:56:15.270415Z","iopub.status.idle":"2021-10-17T17:56:16.610859Z","shell.execute_reply.started":"2021-10-17T17:56:15.270381Z","shell.execute_reply":"2021-10-17T17:56:16.609811Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### Reload the model","metadata":{}},{"cell_type":"code","source":"reloaded = tf.keras.models.load_model('dnn_model')\n\ntest_results['reloaded'] = reloaded.evaluate(\n    X_train[features], y_train, verbose=0)\n\npd.DataFrame(test_results, index=['Mean absolute error [mpg]']).T","metadata":{"execution":{"iopub.status.busy":"2021-10-17T17:56:19.361855Z","iopub.execute_input":"2021-10-17T17:56:19.362601Z","iopub.status.idle":"2021-10-17T17:56:19.883968Z","shell.execute_reply.started":"2021-10-17T17:56:19.362553Z","shell.execute_reply":"2021-10-17T17:56:19.883034Z"},"trusted":true},"execution_count":60,"outputs":[]}]}